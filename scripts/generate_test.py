import math
from itertools import product
from pathlib import Path

import torch


# RUN BACKPROPAGATION

torch.manual_seed(0)

dims = [2, 3, 4]

inputs = torch.rand(dims[0])
label = torch.rand(dims[-1])

weights = [torch.rand((o, i), requires_grad=True) for (i, o) in zip(dims, dims[1:])]
biases = [torch.rand(dim, requires_grad=True) for dim in dims[1:]]

outputs = inputs
for (w, b) in zip(weights, biases):
    outputs = torch.sigmoid(w @ outputs + b)

loss = (outputs - label).square().sum()
loss.backward()

# CODE GENERATION

make_network = ()

square_brackets = lambda indices, shape: " + ".join(
    f"{idx} * {math.prod(shape[dim + 1:])}" for (dim, idx) in enumerate(indices)
)

fill_array = lambda name, tensor: "\n".join(
    f"{name}[{square_brackets(indices, tensor.shape)}] = {tensor[indices]};"
    for indices in product(*map(range, tensor.shape))
)

alloc_array = (
    lambda name, tensor: f"double *{name} = malloc({tensor.numel()} * sizeof(double));"
)

weights_grad = [(f"nabla_w{i}", w.grad) for (i, w) in enumerate(weights, 1)]
biases_grad = [(f"nabla_b{i}", b.grad) for (i, b) in enumerate(biases, 1)]

gradients = weights_grad + biases_grad

code = [
    "// create network",
    f"int dims[] = {{{', '.join(map(str,dims))}}};",
    f"Network network = network_create({len(dims)}, dims);",
    "",
    "// fill network",
    *(
        fill_array(f"network.{name}", tensor)
        for (name, tensor) in [
            *((f"weights[{i}]", w) for (i, w) in enumerate(weights, 1)),
            *((f"biases[{i}]", b) for (i, b) in enumerate(biases, 1)),
        ]
    ),
    "",
    "// fill gradients",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in gradients
    ),
    "",
    "// fill inputs and label",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in [("inputs", inputs), ("label", label)]
    ),
    "",
    "// run backprop",
    "forward(network, inputs);",
    "double loss = compute_loss(network, label);",
    "backward(network, label);",
    "// compare loss",
    f'assert_scalar("loss", {float(loss)}, loss);',
    "",
    "// compare gradients",
    *(
        f'assert_array("{name}", {tensor.numel()}, {name}, network.{dict(w="weights", b="biases")[name[-2]]}_grad[{int(name[-1])}]);'
        for (name, tensor) in gradients
    ),
    "",
    "// free gradients",
    *(f"free({name});" for (name, _) in gradients),
    "// free inputs and labels",
    *(f"free({name});" for name in ["inputs", "label"]),
    "// destroy network",
    "network_destroy(network);",
]

indent = lambda text: "\n".join(("    " + line).rstrip() for line in text.split("\n"))

text = [
    f"/* automatically generated by '{Path(__file__).name}' */",
    "void test_back_propagation()\n{",
    indent("\n".join(code)),
    "}\n",
]

(Path(__file__).parent.parent / "src" / "test_backprop.c").write_text("\n".join(text))
