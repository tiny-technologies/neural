import math
from itertools import product
from pathlib import Path

import torch


# RUN BACKPROPAGATION

torch.manual_seed(0)

s0, s1, s2 = 2, 3, 4

inputs = torch.rand(s0)
label = torch.rand(s2)

w1 = torch.rand((s1, s0), requires_grad=True)
w2 = torch.rand((s2, s1), requires_grad=True)
b1 = torch.rand(s1, requires_grad=True)
b2 = torch.rand(s2, requires_grad=True)

outputs = torch.sigmoid(w2 @ torch.sigmoid(w1 @ inputs + b1) + b2)
loss = (outputs - label).square().sum()

loss.backward()

# CODE GENERATION

make_network = (
    f"int ndims[] = {{{s0}, {s1}, {s2}}};\nNetwork network = network_create(3, ndims);"
)

square_brackets = lambda indices, shape: " + ".join(
    f"{idx} * {math.prod(shape[dim + 1:])}" for (dim, idx) in enumerate(indices)
)

fill_array = lambda name, tensor: "\n".join(
    f"{name}[{square_brackets(indices, tensor.shape)}] = {tensor[indices]};"
    for indices in product(*map(range, tensor.shape))
)

alloc_array = (
    lambda name, tensor: f"double *{name} = malloc({tensor.numel()} * sizeof(double));"
)

gradients = [
    ("nabla_w1", w1.grad),
    ("nabla_b1", b1.grad),
    ("nabla_w2", w2.grad),
    ("nabla_b2", b2.grad),
]

code = [
    make_network,
    "",
    "// fill network",
    *(
        fill_array(f"network.{name}", tensor)
        for (name, tensor) in [
            ("weights[1]", w1),
            ("weights[2]", w2),
            ("biases[1]", b1),
            ("biases[2]", b2),
        ]
    ),
    "",
    "// fill gradients",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in gradients
    ),
    "",
    "// fill inputs and label",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in [("inputs", inputs), ("label", label)]
    ),
    "",
    "// run backprop",
    "forward(network, inputs);",
    "double loss = compute_loss(network, label);",
    "backward(network, label);",
    "// compare loss",
    f'assert_scalar("loss", {float(loss)}, loss);',
    "",
    "// compare gradients",
    *(
        f'assert_array("{name}", {tensor.numel()}, {name}, network.{dict(w="weights", b="biases")[name[-2]]}_grad[{int(name[-1])}]);'
        for (name, tensor) in gradients
    ),
    "",
    "// free gradients",
    *(f"free({name});" for (name, _) in gradients),
    "// free inputs and labels",
    *(f"free({name});" for name in ["inputs", "label"]),
]

indent = lambda text: "    " + text.replace("\n", "\n    ")
remove_trailing_whitespace = lambda text: "\n".join(map(str.rstrip, text.split("\n")))

text = "\n".join(
    [
        f"/* automatically generated by '{Path(__file__).name}' */",
        "void test_back_propagation()\n{",
        remove_trailing_whitespace(indent("\n".join(code))),
        "}\n",
    ]
)

(Path(__file__).parent.parent / "src" / "test_backprop.c").write_text(text)
