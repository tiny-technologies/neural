import math
from itertools import product
from pathlib import Path

import torch


# RUN BACKPROPAGATION

torch.manual_seed(0)

dims = [2, 3, 4, 3, 2]

inputs = torch.rand(dims[0])
label = torch.rand(dims[-1])

weights = [torch.rand((o, i), requires_grad=True) for (i, o) in zip(dims, dims[1:])]
biases = [torch.rand(dim, requires_grad=True) for dim in dims[1:]]

outputs = inputs
for (w, b) in zip(weights, biases):
    outputs = torch.sigmoid(w @ outputs + b)

loss = (outputs - label).square().sum()
loss.backward()

# CODE GENERATION

make_network = ()
fill_array = lambda name, tensor: "\n".join(
    f"{name}[{i}] = {tensor[indices]};"
    for (i, indices) in enumerate(product(*map(range, tensor.shape)))
)

alloc_array = (
    lambda name, tensor: f"double *{name} = malloc({tensor.numel()} * sizeof(double));"
)

weights_gradients = [(f"nabla_w{i}", w.grad) for (i, w) in enumerate(weights, 1)]
biases_gradients = [(f"nabla_b{i}", b.grad) for (i, b) in enumerate(biases, 1)]
gradients = weights_gradients + biases_gradients

body = [
    "// create network",
    f"int dims[] = {{{', '.join(map(str,dims))}}};",
    f"Network network = network_create({len(dims)}, dims);",
    "",
    "// fill network",
    *(
        fill_array(f"network.{name}", tensor)
        for (name, tensor) in [
            *((f"weights[{i}]", w) for (i, w) in enumerate(weights, 1)),
            *((f"biases[{i}]", b) for (i, b) in enumerate(biases, 1)),
        ]
    ),
    "",
    "// fill gradients",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in gradients
    ),
    "",
    "// fill inputs and label",
    *(
        alloc_array(name, tensor) + "\n" + fill_array(name, tensor)
        for (name, tensor) in [("inputs", inputs), ("label", label)]
    ),
    "",
    "// run backprop",
    "forward(network, inputs);",
    "double loss = compute_loss(network, label);",
    "backward(network, label);",
    "// compare loss",
    f'assert_scalar("loss", {float(loss)}, loss);',
    "",
    "// compare gradients",
    *(
        f'assert_array("{name}", {tensor.numel()}, {name}, network.{dict(w="weights", b="biases")[name[-2]]}_grad[{int(name[-1])}]);'
        for (name, tensor) in gradients
    ),
    "",
    "// free gradients",
    *(f"free({name});" for (name, _) in gradients),
    "// free inputs and labels",
    *(f"free({name});" for name in ["inputs", "label"]),
    "// destroy network",
    "network_destroy(network);",
]


test = [
    f"/* automatically generated by '{Path(__file__).name}' */",
    "void test_back_propagation()\n{",
    "\n".join(("    " + line).rstrip() for line in "\n".join(body).split("\n")),
    "}\n",
]

(Path(__file__).parent.parent / "src" / "test_backprop.c").write_text("\n".join(test))
